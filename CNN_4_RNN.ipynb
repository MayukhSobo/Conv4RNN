{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:46:47.339374Z",
     "start_time": "2019-02-28T06:46:46.027527Z"
    }
   },
   "outputs": [],
   "source": [
    "# from preprocess import TextReader\n",
    "from train import CNNText\n",
    "\n",
    "from app import _get_available_dev, _buildVocabulary\n",
    "\n",
    "# import pandas as pd\n",
    "# from pyemd import emd\n",
    "import numpy as np\n",
    "# from gensim.models import KeyedVectors\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:CPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_available_dev('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_available_dev('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process the data and convert words into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:54:23.237296Z",
     "start_time": "2019-02-28T06:54:23.088406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranks  rt-polarity.neg\trt-polarity.pos  train\tvalid\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found datafiles with the following class labels {'./data/rt-polarity.pos': 1, './data/rt-polarity.neg': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: ./data/rt-polarity.pos:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created token ranks ./data/ranks of size 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: ./data/rt-polarity.neg: 100%|██████████| 2/2 [00:05<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training data of shape (10662, 51)\n",
      "Created training label of shape (10662,)\n",
      "Saved the train and test frames in ./data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_buildVocabulary(base_path='./data/',\n",
    "                 clean=True,\n",
    "                 max_vocab=15000,\n",
    "                 shuffle=True,\n",
    "                 test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranks  rt-polarity.neg\trt-polarity.pos  train\tvalid\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./data/train/X_train.npy')\n",
    "X_valid = np.load('./data/valid/X_valid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load('./data/train/y_train.npy')\n",
    "y_valid = np.load('./data/valid/y_valid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9595, 51)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1067, 51)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9595,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1067,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the batch Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir TFLOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:55:35.532301Z",
     "start_time": "2019-02-28T06:55:35.521354Z"
    }
   },
   "outputs": [],
   "source": [
    "cnnText = CNNText(\n",
    "    train_path='./train/',\n",
    "    valid_path='./valid/',\n",
    "    epochs=5,\n",
    "    batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:56:18.367908Z",
     "start_time": "2019-02-28T06:55:38.735327Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10/955 (epoch 1/5), loss = 8.800632 (859.5 examples/sec; 0.058 sec/batch), lr: 0.010000\n",
      "step 20/955 (epoch 1/5), loss = 5.031693 (948.2 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 30/955 (epoch 1/5), loss = 2.993293 (982.7 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 40/955 (epoch 1/5), loss = 1.927647 (920.8 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 50/955 (epoch 1/5), loss = 1.323100 (971.1 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 60/955 (epoch 1/5), loss = 1.041734 (967.9 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 70/955 (epoch 1/5), loss = 0.864123 (988.2 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 80/955 (epoch 1/5), loss = 0.772862 (980.7 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 90/955 (epoch 1/5), loss = 0.761071 (928.9 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 100/955 (epoch 1/5), loss = 0.722306 (941.6 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 110/955 (epoch 1/5), loss = 0.700774 (957.5 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 120/955 (epoch 1/5), loss = 0.713404 (972.6 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 130/955 (epoch 1/5), loss = 0.685518 (1020.1 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 140/955 (epoch 1/5), loss = 0.689163 (1003.0 examples/sec; 0.050 sec/batch), lr: 0.010000\n",
      "step 150/955 (epoch 1/5), loss = 0.683668 (919.7 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 160/955 (epoch 1/5), loss = 0.705887 (953.5 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 170/955 (epoch 1/5), loss = 0.696504 (967.3 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 180/955 (epoch 1/5), loss = 0.699959 (948.0 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 190/955 (epoch 1/5), loss = 0.702026 (1024.2 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "Epoch 1: training_loss = 1.923665, training_accuracy = 0.646\n",
      "Epoch 1: test_loss = 0.684854, test_accuracy = 0.712\n",
      "step 200/955 (epoch 2/5), loss = 0.700201 (1040.1 examples/sec; 0.048 sec/batch), lr: 0.010000\n",
      "step 210/955 (epoch 2/5), loss = 0.700183 (978.9 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 220/955 (epoch 2/5), loss = 0.687443 (989.6 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 230/955 (epoch 2/5), loss = 0.700666 (903.1 examples/sec; 0.055 sec/batch), lr: 0.010000\n",
      "step 240/955 (epoch 2/5), loss = 0.695611 (940.7 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 250/955 (epoch 2/5), loss = 0.675919 (942.3 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 260/955 (epoch 2/5), loss = 0.690401 (970.9 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 270/955 (epoch 2/5), loss = 0.703540 (911.0 examples/sec; 0.055 sec/batch), lr: 0.010000\n",
      "step 280/955 (epoch 2/5), loss = 0.661845 (1010.7 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 290/955 (epoch 2/5), loss = 0.702899 (964.1 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 300/955 (epoch 2/5), loss = 0.698517 (1027.9 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 310/955 (epoch 2/5), loss = 0.708358 (1029.5 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 320/955 (epoch 2/5), loss = 0.689830 (959.2 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 330/955 (epoch 2/5), loss = 0.686706 (985.1 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 340/955 (epoch 2/5), loss = 0.680733 (973.9 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 350/955 (epoch 2/5), loss = 0.670915 (992.5 examples/sec; 0.050 sec/batch), lr: 0.010000\n",
      "step 360/955 (epoch 2/5), loss = 0.694263 (1011.3 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 370/955 (epoch 2/5), loss = 0.684921 (976.9 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 380/955 (epoch 2/5), loss = 0.677369 (989.3 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "Epoch 2: training_loss = 0.688137, training_accuracy = 0.703\n",
      "Epoch 2: test_loss = 0.683809, test_accuracy = 0.719\n",
      "step 390/955 (epoch 3/5), loss = 0.699788 (989.0 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 400/955 (epoch 3/5), loss = 0.685943 (1002.8 examples/sec; 0.050 sec/batch), lr: 0.010000\n",
      "step 410/955 (epoch 3/5), loss = 0.689190 (910.2 examples/sec; 0.055 sec/batch), lr: 0.010000\n",
      "step 420/955 (epoch 3/5), loss = 0.675104 (973.5 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 430/955 (epoch 3/5), loss = 0.677889 (965.6 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 440/955 (epoch 3/5), loss = 0.695070 (1012.5 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 450/955 (epoch 3/5), loss = 0.674908 (979.7 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 460/955 (epoch 3/5), loss = 0.675560 (997.0 examples/sec; 0.050 sec/batch), lr: 0.010000\n",
      "step 470/955 (epoch 3/5), loss = 0.704989 (959.2 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 480/955 (epoch 3/5), loss = 0.683652 (1032.7 examples/sec; 0.048 sec/batch), lr: 0.010000\n",
      "step 490/955 (epoch 3/5), loss = 0.684608 (932.7 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 500/955 (epoch 3/5), loss = 0.652726 (878.9 examples/sec; 0.057 sec/batch), lr: 0.010000\n",
      "step 510/955 (epoch 3/5), loss = 0.705704 (1024.1 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 520/955 (epoch 3/5), loss = 0.683253 (974.9 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 530/955 (epoch 3/5), loss = 0.664559 (912.2 examples/sec; 0.055 sec/batch), lr: 0.010000\n",
      "step 540/955 (epoch 3/5), loss = 0.694716 (957.6 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 550/955 (epoch 3/5), loss = 0.692193 (1000.7 examples/sec; 0.050 sec/batch), lr: 0.010000\n",
      "step 560/955 (epoch 3/5), loss = 0.678410 (864.7 examples/sec; 0.058 sec/batch), lr: 0.010000\n",
      "step 570/955 (epoch 3/5), loss = 0.712750 (978.9 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "Epoch 3: training_loss = 0.687540, training_accuracy = 0.708\n",
      "Epoch 3: test_loss = 0.683284, test_accuracy = 0.719\n",
      "step 580/955 (epoch 4/5), loss = 0.713304 (911.0 examples/sec; 0.055 sec/batch), lr: 0.010000\n",
      "step 590/955 (epoch 4/5), loss = 0.685472 (900.4 examples/sec; 0.056 sec/batch), lr: 0.010000\n",
      "step 600/955 (epoch 4/5), loss = 0.697716 (1025.2 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 610/955 (epoch 4/5), loss = 0.665332 (943.7 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 620/955 (epoch 4/5), loss = 0.668542 (990.8 examples/sec; 0.050 sec/batch), lr: 0.010000\n",
      "step 630/955 (epoch 4/5), loss = 0.699916 (921.5 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 640/955 (epoch 4/5), loss = 0.682560 (968.0 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 650/955 (epoch 4/5), loss = 0.684857 (927.8 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 660/955 (epoch 4/5), loss = 0.658267 (980.2 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 670/955 (epoch 4/5), loss = 0.679835 (953.0 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 680/955 (epoch 4/5), loss = 0.705124 (1016.7 examples/sec; 0.049 sec/batch), lr: 0.010000\n",
      "step 690/955 (epoch 4/5), loss = 0.683211 (961.2 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 700/955 (epoch 4/5), loss = 0.688807 (934.9 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 710/955 (epoch 4/5), loss = 0.691825 (925.0 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 720/955 (epoch 4/5), loss = 0.686493 (904.0 examples/sec; 0.055 sec/batch), lr: 0.010000\n",
      "step 730/955 (epoch 4/5), loss = 0.706673 (831.8 examples/sec; 0.060 sec/batch), lr: 0.010000\n",
      "step 740/955 (epoch 4/5), loss = 0.682352 (972.8 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 750/955 (epoch 4/5), loss = 0.692254 (903.7 examples/sec; 0.055 sec/batch), lr: 0.010000\n",
      "step 760/955 (epoch 4/5), loss = 0.667624 (923.3 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "Epoch 4: training_loss = 0.687438, training_accuracy = 0.708\n",
      "Epoch 4: test_loss = 0.682858, test_accuracy = 0.719\n",
      "step 770/955 (epoch 5/5), loss = 0.703237 (957.5 examples/sec; 0.052 sec/batch), lr: 0.010000\n",
      "step 780/955 (epoch 5/5), loss = 0.707242 (919.4 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 790/955 (epoch 5/5), loss = 0.682163 (1008.1 examples/sec; 0.050 sec/batch), lr: 0.010000\n",
      "step 800/955 (epoch 5/5), loss = 0.683524 (881.1 examples/sec; 0.057 sec/batch), lr: 0.010000\n",
      "step 810/955 (epoch 5/5), loss = 0.687943 (924.1 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 820/955 (epoch 5/5), loss = 0.671580 (951.8 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 830/955 (epoch 5/5), loss = 0.695659 (940.8 examples/sec; 0.053 sec/batch), lr: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 840/955 (epoch 5/5), loss = 0.673110 (867.1 examples/sec; 0.058 sec/batch), lr: 0.010000\n",
      "step 850/955 (epoch 5/5), loss = 0.688016 (879.5 examples/sec; 0.057 sec/batch), lr: 0.010000\n",
      "step 860/955 (epoch 5/5), loss = 0.692029 (881.7 examples/sec; 0.057 sec/batch), lr: 0.010000\n",
      "step 870/955 (epoch 5/5), loss = 0.664616 (883.3 examples/sec; 0.057 sec/batch), lr: 0.010000\n",
      "step 880/955 (epoch 5/5), loss = 0.666635 (941.8 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 890/955 (epoch 5/5), loss = 0.693246 (989.8 examples/sec; 0.051 sec/batch), lr: 0.010000\n",
      "step 900/955 (epoch 5/5), loss = 0.710077 (874.0 examples/sec; 0.057 sec/batch), lr: 0.010000\n",
      "step 910/955 (epoch 5/5), loss = 0.692272 (894.9 examples/sec; 0.056 sec/batch), lr: 0.010000\n",
      "step 920/955 (epoch 5/5), loss = 0.677874 (880.2 examples/sec; 0.057 sec/batch), lr: 0.010000\n",
      "step 930/955 (epoch 5/5), loss = 0.673888 (924.3 examples/sec; 0.054 sec/batch), lr: 0.010000\n",
      "step 940/955 (epoch 5/5), loss = 0.690818 (938.7 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "step 950/955 (epoch 5/5), loss = 0.683335 (951.8 examples/sec; 0.053 sec/batch), lr: 0.010000\n",
      "Epoch 5: training_loss = 0.687101, training_accuracy = 0.708\n",
      "Epoch 5: test_loss = 0.682672, test_accuracy = 0.719\n"
     ]
    }
   ],
   "source": [
    "cnnText.train(tolerance=0.01, logdir='./TFLOGS/',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
