{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pyemd import emd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.externals import joblib\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin',\n",
    "                                         binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextReader:\n",
    "    \"\"\"\n",
    "    A class meant to load the text data from \n",
    "    files distinctively identifiable for different\n",
    "    class labels, clean the text and use pretrained\n",
    "    word vectors to convert into suitable word vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, suffix_labels):\n",
    "        self.path = data_dir\n",
    "        self.ranks = None\n",
    "        self.raw_labeled_data = defaultdict(list)\n",
    "        self.word_fequency = None\n",
    "        self.max_text_length = 0\n",
    "        self.data_files = {}\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        for file, label in suffix_labels.items():\n",
    "            if not os.path.exists(os.path.join(data_dir, file)) or not \\\n",
    "            os.path.isfile(os.path.join(data_dir, file)):\n",
    "                raise IOError(f'Data files are not found in {data_dir}')\n",
    "            else:\n",
    "                self.data_files[os.path.join(data_dir, file)] = label\n",
    "    \n",
    "    def clean_text(self, text, stopwords):\n",
    "        \"\"\"\n",
    "        Cleaning the text\n",
    "        \"\"\"\n",
    "        text = \" \".join(filter(lambda x: all([x.isalpha(), x not in stopwords]), \n",
    "                               word_tokenize(text)))\n",
    "        return text.strip().lower()\n",
    "    \n",
    "    def prepare_data(self, clean=True, **kwargs):\n",
    "        all_words = []\n",
    "        for file_path, class_label in self.data_files.items():\n",
    "            lines = []\n",
    "            with open(file_path, 'r', encoding='latin-1') as infile:\n",
    "                for line in infile:\n",
    "                    if not clean:\n",
    "                        cleaned_line = line\n",
    "                    else:\n",
    "                        stopwords = kwargs.get('stopwords', [])\n",
    "                        cleaned_line = self.clean_text(line, stopwords)\n",
    "\n",
    "                    lines.append(cleaned_line)\n",
    "                    tokens = cleaned_line.split()\n",
    "                    self.max_text_length = max(self.max_text_length, len(tokens))\n",
    "                    all_words.extend(tokens)\n",
    "                    self.raw_labeled_data[class_label].append(cleaned_line)\n",
    "        \n",
    "        self.word_fequency = Counter(all_words)\n",
    "        return self.store_ranking(kwargs.get('max_vocab'))\n",
    "    \n",
    "    def store_ranking(self, max_vocab=None):\n",
    "        ranks = [*map(lambda x: x[0], self.word_fequency.most_common(max_vocab))]\n",
    "        np.save(os.path.join(self.path, 'ranks'), ranks)\n",
    "        return True\n",
    "    \n",
    "    def get_rank(self, token):\n",
    "        if self.ranks is None:\n",
    "            self.ranks = np.load(os.path.join(self.path, 'ranks.npy'))\n",
    "        try:\n",
    "            return int(np.where(self.ranks == token)[0][0]) + 1\n",
    "        except IndexError:\n",
    "            return 0\n",
    "            \n",
    "    def get_ranked_features(self):\n",
    "        if self.X is not None and self.y is not None:\n",
    "            return self.X, self.y\n",
    "        X = []\n",
    "        y = []\n",
    "        for label, corpus in self.raw_labeled_data.items():\n",
    "            for doc in tqdm(corpus):\n",
    "                tokens = doc.split()\n",
    "                ranks = [self.get_rank(token) for token in tokens]\n",
    "                pad_left = (self.max_text_length - len(tokens)) // 2\n",
    "                pad_right = int(np.ceil((self.max_text_length - len(tokens)) / 2.0))\n",
    "                ranks = np.pad(ranks, pad_width=(pad_left, pad_right), \n",
    "                               mode='constant', constant_values=(-1, -1))\n",
    "                y.append(label)\n",
    "                X.append(ranks)\n",
    "        return np.array(X, dtype=int), np.array(y, dtype=int)\n",
    "    \n",
    "    def get_embedding_vector(self, model):\n",
    "        \"\"\"\n",
    "        Get the embedding vector from the model.\n",
    "        We can use pretrained word vectors like Google News.\n",
    "        \"\"\"\n",
    "        for word in self.word_fequency:\n",
    "            if model.__contains__(word):\n",
    "                yield word, model[word]\n",
    "            else:\n",
    "                yield word, np.random.uniform(-0.25, 0.25, model.vector_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = TextReader(data_dir='./', suffix_labels={'rt-polarity.pos': 1, 'rt-polarity.neg': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./rt-polarity.pos': 1, './rt-polarity.neg': 0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.prepare_data(clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5331/5331 [00:16<00:00, 318.09it/s]\n",
      "100%|██████████| 5331/5331 [00:16<00:00, 327.16it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = tr.get_ranked_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = tr.get_embedding_vector(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "for word, vector in wv:\n",
    "    word_vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_df = pd.DataFrame.from_dict(word_vectors, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.118652</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>-0.030151</td>\n",
       "      <td>-0.013000</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.047607</td>\n",
       "      <td>-0.068848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rock</th>\n",
       "      <td>0.106445</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>-0.018433</td>\n",
       "      <td>0.186523</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>-0.009827</td>\n",
       "      <td>-0.392578</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097168</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>-0.300781</td>\n",
       "      <td>0.014771</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>-0.008057</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.091797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233398</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>destined</th>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.462891</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>-0.040527</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>0.212891</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.090332</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031738</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>0.045166</td>\n",
       "      <td>-0.012573</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>-0.078125</td>\n",
       "      <td>-0.103516</td>\n",
       "      <td>-0.206055</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>-0.136719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.047481</td>\n",
       "      <td>-0.142570</td>\n",
       "      <td>-0.067143</td>\n",
       "      <td>-0.128437</td>\n",
       "      <td>-0.142369</td>\n",
       "      <td>-0.221884</td>\n",
       "      <td>-0.118976</td>\n",
       "      <td>0.207664</td>\n",
       "      <td>0.056873</td>\n",
       "      <td>0.023665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038359</td>\n",
       "      <td>-0.100123</td>\n",
       "      <td>0.022912</td>\n",
       "      <td>-0.199677</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>-0.078890</td>\n",
       "      <td>-0.024419</td>\n",
       "      <td>0.075134</td>\n",
       "      <td>0.204857</td>\n",
       "      <td>-0.236962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5    \\\n",
       "the       0.080078  0.104980  0.049805  0.053467 -0.067383 -0.120605   \n",
       "rock      0.106445  0.005920 -0.018433  0.186523 -0.065430 -0.115723   \n",
       "is        0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242   \n",
       "destined  0.197266  0.462891  0.004974  0.168945 -0.040527  0.053467   \n",
       "to       -0.047481 -0.142570 -0.067143 -0.128437 -0.142369 -0.221884   \n",
       "\n",
       "               6         7         8         9      ...          290  \\\n",
       "the       0.035156 -0.118652  0.043945  0.030151    ...    -0.071289   \n",
       "rock     -0.009827 -0.392578  0.010010  0.091797    ...     0.097168   \n",
       "is        0.112793 -0.107910  0.071777  0.020874    ...    -0.233398   \n",
       "destined  0.212891 -0.096680  0.090332  0.312500    ...    -0.031738   \n",
       "to       -0.118976  0.207664  0.056873  0.023665    ...     0.038359   \n",
       "\n",
       "               291       292       293       294       295       296  \\\n",
       "the      -0.030151 -0.013000  0.016357 -0.018311  0.014832  0.005005   \n",
       "rock      0.171875 -0.300781  0.014771 -0.120605 -0.008057  0.071289   \n",
       "is       -0.036377 -0.093750  0.182617  0.027100  0.127930 -0.024780   \n",
       "destined -0.112305  0.045166 -0.012573  0.291016 -0.078125 -0.103516   \n",
       "to       -0.100123  0.022912 -0.199677  0.001070 -0.078890 -0.024419   \n",
       "\n",
       "               297       298       299  \n",
       "the       0.003662  0.047607 -0.068848  \n",
       "rock      0.026367  0.081543  0.091797  \n",
       "is        0.011230  0.164062  0.106934  \n",
       "destined -0.206055  0.195312 -0.136719  \n",
       "to        0.075134  0.204857 -0.236962  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_df.to_csv('./word_vectors.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_4_RNN.ipynb  ranks.npy  rt-polarity.neg  word_vectors.csv\r\n",
      "nohup.out\t README.md  rt-polarity.pos\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
